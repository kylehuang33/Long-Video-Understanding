#!/bin/bash

# QA using existing captions via vLLM server
# Make sure vLLM server is running before executing this script!
# Start vLLM server with: vllm serve Qwen/Qwen3-VL-4B-Instruct --port 8000

# IMPORTANT: Run run_caption.sh first to generate captions!

PARQUET_PATH="/mnt/data-alpha-sg-02/team-agent/ai_glasses/datasets/ViRL39K/39Krelease.parquet"
DATASET_ROOT="/mnt/data-alpha-sg-02/team-agent/ai_glasses/datasets/ViRL39K"
OUTPUT_DIR="./results_caption"
VLLM_URL="http://localhost:8000/v1"
QA_MODEL="Qwen/Qwen3-VL-4B-Instruct"

# Caption file to use (must exist - generated by run_caption.sh)
# Format: {OUTPUT_DIR}/captions/{caption_model}/{caption_model}_{prompt_style}.json
CAPTION_FILE="./results_caption/captions/Qwen3-VL-4B-Instruct/Qwen3-VL-4B-Instruct_simple.json"

# Check if caption file exists
if [ ! -f "$CAPTION_FILE" ]; then
    echo "Error: Caption file not found: $CAPTION_FILE"
    echo "Please run run_caption.sh first to generate captions!"
    exit 1
fi

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Run QA with captions
# Output will be saved to: {OUTPUT_DIR}/qa_results/{qa_model}/{qa_model}_with_{caption_model}_{style}.json
# Example: ./results_caption/qa_results/Qwen3-VL-4B-Instruct/Qwen3-VL-4B-Instruct_with_Qwen3-VL-4B-Instruct_simple.json
python qa_with_captions_vllm.py \
    --parquet-path "$PARQUET_PATH" \
    --dataset-root "$DATASET_ROOT" \
    --caption-file "$CAPTION_FILE" \
    --output-dir "$OUTPUT_DIR" \
    --vllm-url "$VLLM_URL" \
    --model "$QA_MODEL" \
    --max-tokens 512 \
    --temperature 0.0

echo "Done! Check $OUTPUT_DIR/qa_results for QA results"
